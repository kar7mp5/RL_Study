{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf5947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\", continuous=False, gravity=-10.0,\n",
    "               enable_wind=False, wind_power=15.0, turbulence_power=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c7e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import argparse, pdb\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from typing import Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17a8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque(maxlen=maxlen)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e282e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc_out = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, maxlen=50000):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # 하이퍼 파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 5e-4\n",
    "        self.epsilon_start = 0.9\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 10000\n",
    "        self.batch_size = 64\n",
    "        self.tau = 0.005\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = 0\n",
    "        self.loss_val = 0\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Policy model과 target model 따로 정의 (파라미터 초기화는 동일하게)\n",
    "        self.policy_net = DQN(self.state_size, self.action_size).to(self.device)\n",
    "        self.target_net = DQN(self.state_size, self.action_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.memory = ReplayMemory(maxlen=maxlen)\n",
    "\n",
    "        # Huber loss\n",
    "        self.lossfn = nn.SmoothL1Loss()\n",
    "\n",
    "    # Epsilon-greedy (epsilon 지수 감쇠 업데이트)\n",
    "    # 입력: ndarray (1, state_size), 출력: int\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) *\\\n",
    "                    math.exp(-1 * self.steps_done / self.epsilon_decay)       \n",
    "        self.steps_done += 1\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else :\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_val = self.policy_net(state)\n",
    "            return torch.argmax(q_val[0]).item()\n",
    "\n",
    "    # 배치차원 추가 후 memory 저장\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor([state], dtype=torch.float32, device=self.device)\n",
    "        action = torch.tensor([[action]], dtype=torch.long, device=self.device)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        if not done:\n",
    "            next_state = torch.tensor([next_state], dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            next_state = None\n",
    "        self.memory.push(state, action, reward, next_state)\n",
    "\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Transition 객체 batch list\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # batch.state > batch state 텐서 담긴 튜플\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                      device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "\n",
    "        target_values = (next_state_values * self.discount_factor) + reward_batch\n",
    "\n",
    "        loss = self.lossfn(state_action_values, target_values.unsqueeze(1))\n",
    "        self.loss_val = loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 50)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        target_net_state_dict = self.target_net.state_dict()\n",
    "        policy_net_state_dict = self.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "        self.target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94007d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee\\PythonWorkspace\\RL\\saved_models\\dqn\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "save_dir = root + '\\saved_models\\dqn'\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bbe5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 27648 into shape (1,96)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# env 초기화\u001b[39;00m\n\u001b[0;32m     15\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 16\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# 현재 상태에 대한 행동 선택\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n",
      "File \u001b[1;32mc:\\Users\\Lee\\anaconda3\\envs\\rl_env\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:299\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lee\\anaconda3\\envs\\rl_env\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 27648 into shape (1,96)"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=False, gravity=-10.0,\n",
    "               enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "scores, episodes = [], []\n",
    "EPISODES = 2000\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    # env 초기화\n",
    "    state, info = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "        # 현재 상태에 대한 행동 선택\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        agent.append_memory(state[0], action, reward, next_state[0], done)\n",
    "\n",
    "        # 샘플로 모델 학습\n",
    "        agent.train_model()\n",
    "        \n",
    "        agent.update_target_net()\n",
    "        loss = agent.loss_val\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"episode: {e:4d}, score: {score:2f}, loss: {loss:2f}, epsilon: {agent.epsilon:.3f}\")\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "\n",
    "    # 100 에피소드마다 모델 저장\n",
    "    if e > 0 and e % 100 == 0:\n",
    "        file_name = f\"lunar_lander_dqn_ep{e}.pth\"\n",
    "        save_path = os.path.join(save_dir, file_name)\n",
    "        torch.save(agent.policy_net.state_dict(), save_path)\n",
    "        print(f\"--- Model saved at {save_path} ---\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, scores, 'b')\n",
    "plt.title(\"Lunar Lander - DQN Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdebeb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lee\\AppData\\Local\\Temp\\ipykernel_13044\\1583390402.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Score = 275.76\n",
      "Episode 2: Score = 305.04\n",
      "Episode 3: Score = 271.00\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = save_dir + \"\\lunar_lander_dqn_ep1800.pth\"\n",
    "\n",
    "episodes_n = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", continuous=False, gravity=-10.0,\n",
    "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode=\"human\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "model = DQN(state_size, action_size).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "for e in range(episodes_n):\n",
    "    state, info = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "        \n",
    "        action = torch.argmax(q_values).item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        score += reward\n",
    "        state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "    print(f\"Episode {e+1}: Score = {score:.2f}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
