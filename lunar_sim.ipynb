{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:13:30.136619558Z",
     "start_time": "2025-12-03T13:13:27.392413973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import time"
   ],
   "id": "f8552fa016e45ee7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T13:32:27.525344840Z",
     "start_time": "2025-12-03T13:13:30.138464994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 벡터화된 환경 만들기 (학습 빠르게!)\n",
    "env = make_vec_env(\"LunarLander-v3\", n_envs=8, seed=42)  # 8개 병렬 학습\n",
    "\n",
    "# PPO 모델 생성 (이 하이퍼파라미터가 지금 제일 잘 됨)\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",           # MLP 네트워크\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=1000,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    tensorboard_log=None\n",
    ")\n",
    "\n",
    "# 학습 시작 (총 100만 스텝 ≈ 10~15분 정도면 착륙 성공함!)\n",
    "print(\"Train Start! Background training...\")\n",
    "model.learn(total_timesteps=1_000_000, log_interval=10)\n",
    "\n",
    "# 모델 저장\n",
    "model.save(\"ppo_lunar_lander_v3\")\n",
    "print(\"Finished model saved: ppo_lunar_lander_v3.zip\")"
   ],
   "id": "fbb8499f10007048",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "학습 시작! 창 안 띄워도 백그라운드로 학습 중...\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 241         |\n",
      "|    ep_rew_mean          | -56.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 994         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010474881 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 90.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 802         |\n",
      "|    ep_rew_mean          | 71.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 752         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017368045 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.767      |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.81        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    value_loss           | 71.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 400         |\n",
      "|    ep_rew_mean          | 198         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005450211 |\n",
      "|    clip_fraction        | 0.0405      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.1        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 297          |\n",
      "|    ep_rew_mean          | 193          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 795          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 402          |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030424518 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.674       |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.3         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 197          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 332         |\n",
      "|    ep_rew_mean          | 218         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 818         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004045519 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.656      |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48          |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00096    |\n",
      "|    value_loss           | 98.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 326          |\n",
      "|    ep_rew_mean          | 225          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 831          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 577          |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050063995 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.641       |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.3         |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    value_loss           | 60.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 291         |\n",
      "|    ep_rew_mean          | 242         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 848         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 659         |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006300399 |\n",
      "|    clip_fraction        | 0.0683      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 345         |\n",
      "|    ep_rew_mean          | 240         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 842         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 759         |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003858967 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.5         |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | 0.000278    |\n",
      "|    value_loss           | 15.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 270          |\n",
      "|    ep_rew_mean          | 258          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 850          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 846          |\n",
      "|    total_timesteps      | 720000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051107462 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.58         |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    value_loss           | 11.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 288         |\n",
      "|    ep_rew_mean          | 255         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 931         |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004405978 |\n",
      "|    clip_fraction        | 0.053       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.00056    |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 247         |\n",
      "|    ep_rew_mean          | 266         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 1007        |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004393005 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.97        |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -5.04e-05   |\n",
      "|    value_loss           | 12          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 260         |\n",
      "|    ep_rew_mean          | 265         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 120         |\n",
      "|    time_elapsed         | 1084        |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005553782 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.58        |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.000184   |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "학습 끝! 모델 저장됨: ppo_lunar_lander_v3.zip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T14:58:15.346637386Z",
     "start_time": "2025-12-03T14:58:15.139082834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 학습된 모델 로드해서 보기\n",
    "model = PPO.load(\"ppo_lunar_lander_v3\")\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\", enable_wind=False)\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)  # 최적 행동\n",
    "    print(f\"{action}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"에피소드 종료! 리셋...\")\n",
    "        obs, _ = env.reset()\n",
    "        time.sleep(1)  # 착륙 보고 싶으면 잠깐 멈춤\n",
    "\n",
    "env.close()"
   ],
   "id": "8aa59683304f0d2d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PPO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 학습된 모델 로드해서 보기\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPPO\u001B[49m\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mppo_lunar_lander_v3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLunarLander-v3\u001B[39m\u001B[38;5;124m\"\u001B[39m, render_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, enable_wind\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m obs, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PPO' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
